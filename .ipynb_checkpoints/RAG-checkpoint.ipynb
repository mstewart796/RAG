{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23594862-0e4a-49dc-804e-2297202264bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b02220-44fd-4bc0-99bb-1f3cceebcf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063babd5-c3ed-4556-8bcb-afd3f12e9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "MODEL_COMMAND_R = 'command-r'\n",
    "MODEL_COMMAND_R7B = 'command-r7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c5df37-a543-4bdc-890a-da9308a94f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e22f73f-c6ef-4e14-81b9-7add9ce04a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"RAG-docs/*\")\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63d8f7a-4370-454a-9ccd-ce70b05b95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting using chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a0e754-ac8d-4924-8d6e-9075b8754de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: company-info, q-and-a\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ab87f7-a177-4aed-8c7a-1beb11c45570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of chunks, use the documents\n",
    "# doc_types = set(doc.metadata['doc_type'] for doc in documents)\n",
    "# print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b8f863-f5d7-4da4-85b5-263423bf178d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv('../.env',override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d14fabb-c1b1-42cf-a870-ca8c7824ad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mstew\\AppData\\Local\\Temp\\ipykernel_4676\\3409896792.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41d077f-a409-4783-b05f-44ac512dac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a Chroma Datastore already exists - if so, delete the collection to start from scratch\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abf00fc3-882c-4fb0-8adf-763fc9770efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 23 documents\n"
     ]
    }
   ],
   "source": [
    "# Create our Chroma vectorstore!\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b61e057e-82cf-4cf6-a6d6-77250a11dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM\n",
    "llm = OllamaLLM(model=MODEL_COMMAND_R7B)\n",
    "\n",
    "# Define memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Define the retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}) \n",
    "\n",
    "# Define a custom system message\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"Você é um especialista em responder perguntas precisas sobre a WiseDB e suas soluções de banco de dados e nuvem, da qual você é um funcionário.\n",
    "    Dê respostas precisas e com código quando for disponível nos documentos.\n",
    "    Atenda de forma simpática e personalizada, como um bom atendente de clientes.\n",
    "    Se não souber a resposta, diga isso. Não invente informações se não tiver contexto relevante.\n",
    "    Você responde em português. Mesmo que a pergunta seja feita em inglês, você SEMPRE responde em português.\n",
    "    Comece cada conversa nova agradecendo à pessoa por entrar em contato com a WiseDB. Não repita isso depois.\n",
    "    Se você não conseguir resolver o problema, peça para ligarem para (81) 3036-6577 para falar com um especialista em suporte técnico.\n",
    "            \n",
    "    Aqui está o contexto relevante para ajudar a responder: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the human message template\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "\n",
    "# Create a chat prompt template with both the system and human messages\n",
    "custom_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "# Create the conversation chain with a custom prompt\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": custom_prompt}  # Pass the fixed prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c18bc83-fa4e-408e-8f0e-c22208783ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, \n",
    "                                                           retriever=retriever, \n",
    "                                                           memory=memory,\n",
    "                                                          combine_docs_chain_kwargs={\"prompt\": custom_prompt}\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "708f5e18-8887-4a28-a09f-720a8fc7caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    # Retrieve stored memory\n",
    "    chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "    # Invoke the conversation chain with chat history\n",
    "    result = conversation_chain.invoke({\n",
    "        \"question\": message,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "\n",
    "    # Store the latest interaction in memory\n",
    "    memory.save_context({\"question\": message}, {\"answer\": result[\"answer\"]})\n",
    "\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46075333-bae0-4017-b462-6817cab849bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b38d5-a671-4bbe-a252-1be217f0c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Você tem acesso ao histórico da conversa. Se o usuário perguntar sobre sua primeira pergunta, \n",
    "#     consulte o histórico e forneça a resposta corretamente.\n",
    "#     Histórico da conversa:\n",
    "#     {chat_history}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
